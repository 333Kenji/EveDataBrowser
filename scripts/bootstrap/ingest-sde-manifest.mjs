#!/usr/bin/env node

import { createReadStream } from "node:fs";
import { mkdir, readFile, writeFile } from "node:fs/promises";
import path from "node:path";
import process from "node:process";
import readline from "node:readline";
import { fileURLToPath } from "node:url";
import { Pool } from "pg";

const scriptDir = path.dirname(fileURLToPath(import.meta.url));
const repoRoot = path.resolve(scriptDir, "..", "..");

function printHelp() {
  console.log(`Usage: node scripts/bootstrap/ingest-sde-manifest.mjs --manifest <path> [options]\n\nOptions:\n  --manifest <path>      Manifest JSON generated by generate-sde-manifest.mjs (required)\n  --database-url <url>   Postgres connection string (default: postgresql://eveapp:eveapp@db:5432/eveapp)\n  --batch-size <number>  Number of rows inserted per round trip (default: 1000)\n  --archive-dir <path>   Directory for .ingested markers (default: logs/staging-archive)\n  --report <path>        Write a JSON summary report to the provided path\n  --dry-run              Parse files and report counts without writing to Postgres\n  -h, --help             Show this help message`);
}

function parseArgs(argv) {
  const args = {
    manifest: null,
    databaseUrl: "postgresql://eveapp:eveapp@db:5432/eveapp",
    batchSize: 1000,
    archiveDir: "logs/staging-archive",
    dryRun: false,
    report: null,
  };

  for (let i = 2; i < argv.length; i += 1) {
    const current = argv[i];
    const next = argv[i + 1];

    switch (current) {
      case "--manifest":
        if (!next) throw new Error("Missing value for --manifest");
        args.manifest = next;
        i += 1;
        break;
      case "--database-url":
        if (!next) throw new Error("Missing value for --database-url");
        args.databaseUrl = next;
        i += 1;
        break;
      case "--batch-size":
        if (!next) throw new Error("Missing value for --batch-size");
        args.batchSize = Number.parseInt(next, 10);
        if (!Number.isFinite(args.batchSize) || args.batchSize <= 0) {
          throw new Error("--batch-size must be a positive integer");
        }
        i += 1;
        break;
      case "--archive-dir":
        if (!next) throw new Error("Missing value for --archive-dir");
        args.archiveDir = next;
        i += 1;
        break;
      case "--report":
        if (!next) throw new Error("Missing value for --report");
        args.report = next;
        i += 1;
        break;
      case "--dry-run":
        args.dryRun = true;
        break;
      case "-h":
      case "--help":
        printHelp();
        process.exit(0);
        break;
      default: {
        console.warn(`Ignoring unrecognised argument: ${current}`);
      }
    }
  }

  if (!args.manifest) {
    throw new Error("--manifest is required");
  }

  return args;
}

function baseNameWithoutExtension(name) {
  return name.replace(/\.jsonl$/i, "");
}

function toSchemaName(fileName) {
  const base = baseNameWithoutExtension(fileName)
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, "_")
    .replace(/^_+|_+$/g, "");
  const schema = base ? `stage_sde_${base}` : "stage_sde_dataset";
  if (!/^[a-z_][a-z0-9_]*$/.test(schema)) {
    throw new Error(`Derived schema name '${schema}' is not a valid identifier`);
  }
  return schema;
}

function quoteIdent(identifier) {
  if (!/^[a-z_][a-z0-9_]*$/.test(identifier)) {
    throw new Error(`Invalid identifier: ${identifier}`);
  }
  return `"${identifier}"`;
}

async function ensureRawTable(client, schemaName) {
  const schemaIdent = quoteIdent(schemaName);
  const tableIdent = `${schemaIdent}."raw"`;
  const indexName = `${schemaName}_raw_batch_idx`;
  const sql = `
    CREATE SCHEMA IF NOT EXISTS ${schemaIdent};
    CREATE TABLE IF NOT EXISTS ${tableIdent} (
      id SERIAL PRIMARY KEY,
      ingest_batch UUID NOT NULL,
      ingested_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
      payload JSONB NOT NULL,
      source_file TEXT NOT NULL,
      source_offset BIGINT NOT NULL
    );
    CREATE INDEX IF NOT EXISTS ${quoteIdent(indexName)} ON ${tableIdent} (ingest_batch);
  `;
  await client.query(sql);
}

async function insertBatch(client, tableIdent, batch) {
  if (batch.length === 0) return;

  const batchIds = batch.map((row) => row.batchId);
  const payloads = batch.map((row) => row.payload);
  const files = batch.map((row) => row.sourceFile);
  const offsets = batch.map((row) => row.offset);

  const sql = `
    INSERT INTO ${tableIdent} (ingest_batch, payload, source_file, source_offset)
    SELECT
      batch_id,
      payload::jsonb,
      source_file,
      source_offset
    FROM unnest($1::uuid[], $2::text[], $3::text[], $4::bigint[]) AS t(
      batch_id,
      payload,
      source_file,
      source_offset
    );
  `;

  await client.query(sql, [batchIds, payloads, files, offsets]);
}

async function ingestFile({
  pool,
  manifestPath,
  entry,
  batchSize,
  archiveDir,
  dryRun,
}) {
  const client = dryRun ? null : await pool.connect();
  let batchId = null;
  const schemaName = toSchemaName(entry.name);
  const schemaIdent = quoteIdent(schemaName);
  const tableIdent = `${schemaIdent}."raw"`;
  const absoluteFilePath = path.resolve(process.cwd(), entry.relativePath);
  const relativeFilePath = path.relative(process.cwd(), absoluteFilePath);
  const startedAt = new Date().toISOString();
  const startedHr = process.hrtime.bigint();
  let offset = 0;
  let inserted = 0n;

  try {
    if (dryRun) {
      console.log(`(dry-run) Validating ${relativeFilePath}`);
    } else {
      await ensureRawTable(client, schemaName);
      const { rows } = await client.query(
        "SELECT sde_master.start_ingest_batch($1) AS id",
        [`${manifestPath}#${entry.name}`],
      );
      batchId = rows[0]?.id;
      if (!batchId) {
        throw new Error("Failed to retrieve ingest batch id");
      }
      console.log(`Started batch ${batchId} for ${relativeFilePath}`);
    }

    const stream = createReadStream(absoluteFilePath, { encoding: "utf8" });
    const reader = readline.createInterface({
      input: stream,
      crlfDelay: Infinity,
    });

    const pending = [];
    const progressEvery = 5000;

    if (!dryRun) {
      await client.query("BEGIN");
    }

    for await (const line of reader) {
      const trimmed = line.trim();
      if (!trimmed) {
        // Skip blank lines but maintain offset for deterministic validation
        offset += 1;
        continue;
      }

      offset += 1;

      if (!dryRun) {
        pending.push({
          batchId,
          payload: trimmed,
          sourceFile: relativeFilePath,
          offset,
        });

        if (pending.length >= batchSize) {
          await insertBatch(client, tableIdent, pending);
          inserted += BigInt(pending.length);
          pending.length = 0;
          if (offset % progressEvery === 0) {
            console.log(`  Inserted ${inserted} rows for ${entry.name}...`);
          }
        }
      }
    }

    if (!dryRun && pending.length > 0) {
      await insertBatch(client, tableIdent, pending);
      inserted += BigInt(pending.length);
      pending.length = 0;
    }

    if (!dryRun) {
      const { rows: countRows } = await client.query(
        `SELECT COUNT(*)::bigint AS count FROM ${tableIdent} WHERE ingest_batch = $1`,
        [batchId],
      );
      const dbCount = BigInt(countRows[0]?.count ?? 0);
      const expected = inserted;

      if (dbCount !== expected) {
        throw new Error(
          `Row count mismatch for ${entry.name}: expected ${expected}, inserted ${dbCount}`,
        );
      }

      await client.query("COMMIT");
      const note = `rows=${expected.toString()} file=${relativeFilePath}`;
      await client.query("SELECT sde_master.complete_ingest_batch($1, $2, $3)", [
        batchId,
        "complete",
        note,
      ]);

      await mkdir(archiveDir, { recursive: true });
      const markerPath = path.join(
        archiveDir,
        `${baseNameWithoutExtension(entry.name)}-${batchId}.ingested.json`,
      );
      const marker = {
        manifest: path.relative(process.cwd(), manifestPath),
        file: relativeFilePath,
        batchId,
        rows: expected.toString(),
        completedAt: new Date().toISOString(),
      };
      await writeFile(markerPath, `${JSON.stringify(marker, null, 2)}\n`, "utf8");
      console.log(`Completed ${relativeFilePath} (${expected.toString()} rows)`);
    } else {
      console.log(`(dry-run) Processed ${offset} lines for ${entry.name}`);
    }

    const durationMs = Math.round((Number(process.hrtime.bigint() - startedHr) / 1_000_000) * 100) / 100;
    const completedAt = new Date().toISOString();

    return {
      name: entry.name,
      relativePath: relativeFilePath,
      schema: schemaName,
      dryRun,
      processedLines: offset,
      rowsInserted: dryRun ? null : inserted.toString(),
      batchId,
      startedAt,
      completedAt,
      durationMs,
    };
  } catch (error) {
    if (!dryRun && batchId && client) {
      try {
        await client.query("ROLLBACK");
      } catch {
        // ignore rollback error during failure handling
      }
      const note = error instanceof Error ? error.message : String(error);
      await client.query("SELECT sde_master.complete_ingest_batch($1, $2, $3)", [
        batchId,
        "failed",
        note.slice(0, 250),
      ]);
    } else if (!dryRun && client) {
      try {
        await client.query("ROLLBACK");
      } catch {
        // ignore
      }
    }
    throw error;
  } finally {
    if (client) {
      client.release();
    }
  }
}

async function runSqlFile(pool, relativePath) {
  const sqlPath = path.resolve(repoRoot, relativePath);
  const sql = await readFile(sqlPath, "utf8");
  const client = await pool.connect();
  try {
    await client.query(sql);
  } finally {
    client.release();
  }
}

async function runPostIngestion(pool) {
  const steps = [
    "scripts/bootstrap/create-master-products.sql",
    "scripts/bootstrap/create-master-materials.sql",
    "scripts/bootstrap/create-type-enrichment.sql",
    "scripts/bootstrap/materialize-master-tables.sql",
    "scripts/bootstrap/cleanup-staging.sql",
    "scripts/bootstrap/vacuum-master.sql"
  ];

  for (const step of steps) {
    console.log(`Running ${step}...`);
    await runSqlFile(pool, step);
  }
}

async function main() {
  let args;
  try {
    args = parseArgs(process.argv);
  } catch (error) {
    console.error(error instanceof Error ? error.message : error);
    printHelp();
    process.exit(1);
    return;
  }

  const manifestPath = path.resolve(process.cwd(), args.manifest);
  const manifestRaw = await readFile(manifestPath, "utf8");
  const manifest = JSON.parse(manifestRaw);

  if (!Array.isArray(manifest.files) || manifest.files.length === 0) {
    throw new Error("Manifest contains no files to ingest");
  }

  const pool = new Pool({ connectionString: args.databaseUrl });

  try {
    const summaries = [];

    for (const entry of manifest.files) {
      const summary = await ingestFile({
        pool,
        manifestPath,
        entry,
        batchSize: args.batchSize,
        archiveDir: path.resolve(process.cwd(), args.archiveDir),
        dryRun: args.dryRun,
      });
      summaries.push(summary);
    }

    if (args.report) {
      const reportPath = path.resolve(process.cwd(), args.report);
      await mkdir(path.dirname(reportPath), { recursive: true });
      const report = {
        generatedAt: new Date().toISOString(),
        manifest: path.relative(process.cwd(), manifestPath),
        dryRun: args.dryRun,
        files: summaries,
      };
      await writeFile(reportPath, `${JSON.stringify(report, null, 2)}\n`, "utf8");
      console.log(`Report written to ${path.relative(process.cwd(), reportPath)}`);
    }

    if (!args.dryRun) {
      console.log("Ingestion complete. Materialising master tables and cleaning staging artifacts...");
      await runPostIngestion(pool);
      console.log("Master tables ready and staging assets removed.");
    }
  } finally {
    await pool.end();
  }
}

main().catch((error) => {
  console.error(error instanceof Error ? error.stack ?? error.message : error);
  process.exitCode = 1;
});
